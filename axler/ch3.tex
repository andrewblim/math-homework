\documentclass{article}
\setcounter{section}{2}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[parfill]{parskip}

\begin{document}
\title{Sheldon Axler - Linear Algebra Done Right, 3rd edition - Chapter 3}
\author{Andrew Lim}

\def \problem#1{\subsubsection*{Problem #1}}
\def \real{\mathbf{R}}
\def \complex{\mathbf{C}}
\def \field{\mathbf{F}}

Notations are the same as what Axler uses. For example, we use boldface $\real$
for the reals, and $\field$ is $\real$ or $\complex$, not a general field.

\section{Linear Maps}

\subsection{The Vector Space of Linear Maps}

\problem{1}

$\Rightarrow$: $T(a(x,y,z)) = aT(x,y,z)$ only if $b = c = 0$.

$\Leftarrow$: if $b = c = 0$, then
\begin{align*}
  T(a(x,y,z)) & = (2ax - 4ay - 3az, 6ax) = aT(x,y,z) \\
  T(x_1 + x_2, y_1 + y_2, z_1 + z_2) & = (2(x_1 + x_2) - 4(y_1 + y_2) + 3(z_1 +
                                       z_2), 6(x_1 + x_2)) \\
              & = (2x_1 - 4y_1 + 3z_1, 6x_1) + (2x_2 - 4y_2 + 3z_2, 6x_2) \\
              & = T(x_1, y_1, z_1) + T(x_2, y_2, z_2)
\end{align*}

\problem{2}

$\Rightarrow$: Homogeneity requires $b = 0$, otherwise the $b$ term would
feature $\lambda^2$ for any $p$ where $p(1) \neq 0, p(2) \neq 0$. It also
requires $c = 0$, otherwise for any $p$ where $p(0) \neq 0$, we could pick
$\lambda$ such that $\sin (\lambda p(0)) \neq \lambda \sin (p(0))$.

$\Leftarrow$: if $b = c = 0$, then
\begin{align*}
  T(\lambda p) & = (3 \lambda p(4) + 5 \lambda p'(6) + \int_{-1}^2 x^3 \lambda p(x) dx) \\
               & = \lambda (3p(4) + 5p'(6), \int_{-1}^2 x^3 p(x) dx) \\
               & = \lambda Tp \\
  T(p + q) & = (3(p+q)(4) + 5(p+q)'(6), \int_{-1}^2 x^3(p+q)(x) dx) \\
               & = (3p(4) + 5p'(6) + 3q(4) + 5q'(6), \int_{-1}^2 x^3p(x) dx + \int_{-1}^2 x^3q(x) dx) \\
               & = Tp + Tq
\end{align*}

\problem{3}

Denote the standard basis vectors as $e_i$. Rewrite $(x_1, \ldots, x_n)$ as
$x_1e_1 + \ldots + x_ne_n$. Then, by the two conditions of linearity,
\begin{align*}
  T(x_1, \ldots, x_n) & = \sum_i^n x_iT(e_i)
\end{align*}

We can likewise write $T(e_i)$ in terms of basis vectors $e_i$ in $\field^m$. So
denote:
\begin{align*}
  T(e_i) & = \sum_j^m a_{ji}e_j
\end{align*}

Combining the two and reordering the summation terms, we get:
\begin{align*}
  T(x_1, \ldots, x_n) & = \sum_i^n x_i \sum_j^m a_{ji}e_j \\
                      & = \sum_j^m \sum_i^n a_{ji}x_ie_j
\end{align*}

which is what we want to show, since the last expression above amounts to
putting $a_{j1}x_1 + \ldots + a_{jn}x_n$ into the $j$th element of $T(x_1,
\ldots, x_n)$.

\problem{4}

If the $v_i$ were not linearly independent, then there would be some $a_i$ not
all 0 such that $\sum a_iv_i = 0$. But then $T(\sum a_iv_i) = \sum a_iT(v_i) =
T(0) = 0$, which violates LI of $T(v_i)$.

\problem{5}

Commutativity: $(S+T)(v) = Sv + Tv = Tv + Sv = (T+S)(v)$

Associativity: $(S+(T+U))(v) = Sv + (T+U)v = Sv + Tv + Uv = (S+T)v + Uv =
((S+T) + U(v))$

Additive identity: The zero map $Z(v) = 0$: $(S+Z)(v) = Sv + Zv = Sv = S(v)$.

Additive inverse: For a map $S$, pick inverse $T$ such that $T(v) = -S(v)$. This
is a linear map since $T(u+v) = -S(u+v) = -Su - Sv = T(u) + T(v)$, and since
$T(\lambda v) = -S(\lambda v) = -\lambda Sv = \lambda Tv$. And it is an additive
inverse, since $(S+T)(v) = S(v) + T(v) = 0$, so $S+T$ is the zero map.

Multiplicative identity: the scalar $1 \in \field$: $(1S)(v) = 1(Sv) = Sv$, so
$1S = S$.

Distributive: $(\lambda (S+T))(v) = \lambda ((S+T)(v)) = \lambda (Sv + Tv) =
\lambda Sv + \lambda Tv = (\lambda S)(v) + (\lambda T)(v)$.

\problem{6}

Associativity: $((T_1T_2)T_3)(v) = (T_1T_2)(T_3v) = T_1(T_2(T_3v)) =
T_1((T_2T_3)v) = (T_1(T_2T_3))(v)$.

Identity: $(TI)(v) = T(Iv) = Tv$. Also, $(IT)(v) = I(Tv) = Tv$.

Distributive: $((S_1 + S_2)T)(v) = (S_1 + S_2)(Tv) = S_1(Tv) + S_2T(v) =
(S_1T)(v) + (S_2T)(v)$. Also, $(S(T_1 + T_2))(v) = S((T_1 + T_2)(v)) = S(T_1v +
T_2v) = S(T_1v) + S(T_2v) = (ST_1)(v) + (ST_2)(v)$.

\problem{7}

Pick a basis of $V$, $\{v_1\}$. Then for any linear map $T$, for any $v \in V$
written as $v = av_1$, $T(v) = T(av_1) = aT(v_1)$. Since $T(v_1) \in V$, it can
also be written as $bv_1$ for some scalar $b$, so $T(v) = abv_1 = b(av_1) = bv$,
demonstrating that $T$ is scalar multiplication by $b$.

\problem{8}

Let $\varphi$ be the distance from the origin, taking a positive sign above the
x-axis and a negative sign below:
\begin{align*}
\varphi(x,y) & = \text{sgn}(y)\sqrt{x^2 + y^2}
\end{align*}

Then $\varphi(ax,ay) = a \varphi(x,y)$, but additivity is not satisfied, for
example $\varphi(1,0) + \varphi(0,1) = 1 + 1 = 2$, but $\varphi(1,1) =
\sqrt{2}$.

\problem{9}

\problem{10}

Pick $u$ such that $Su \neq 0$, and $v$ such that $v \notin U$. Then $u+v \notin
U$ either, since otherwise $v$ would be by closure and additive inverses. So
$T(u+v) = 0$. But $T(u) + T(v) = T(u) = S(u) \neq 0$. So $T$ does not satisfy
linearity.

\problem{11}

Consider a basis of $U$ $\{u_i\}$. Extend this to a basis of $V$ with vectors
$\{v_i\}$. Then define $T$ such that $T(u_i) = S(u_i)$ and $T(v_i) = 0$. For any
$u \in U$, clearly $Tu = Su$. We need to show that $T$ is a linear map.

For $v, w \in V$, let their representations in terms of the basis above be $a_i$
and $b_i$, respectively. Then $T(v+w) = T(\sum (a_i + b_i) u_i)$ over the $u_i$
basis vectors only, since the terms with the $v_i$ basis vectors get mapped to
0. This equals $S(\sum (a_i + b_i) u_i)$, since this vector is in $U$. Likewise,
$T(v) + T(w) = T(\sum a_iu_i) + T(\sum b_iu_i) = S(\sum a_iu_i) + S(\sum b_iu_i)
= S(\sum (a_i + b_i) u_i)$, with the last step by linearity of $S$. So $T$ has
additivity.

$T(\lambda v) = T(\lambda \sum a_iu_i) = S(\lambda \sum a_iu_i) = \lambda S(\sum
a_iu_i)$, and $\lambda T(v) = \lambda S(\sum a_iu_i)$, again by similar logic;
the parts of $v$ associated with the $v_i$ disappear, leaving only the parts
associated with $u_i$, which is in $U$ and which satisfy linearity by linearity
of $S$.

\problem{12}

Let $v_i$ be a basis for $V$. Consider some list of linear maps $T_1, \ldots,
T_n$. Then any linear combination of these takes the form $\sum \lambda_i T_i$,
which by linearity maps $v_1$ (the first basis vector of $V$, which must exist
since $\dim V > 0$) to $\lambda_i T_iv_1$.

Because $W$ is infinite-dimensional, the $T_iv_1$ must not span it, so we can
pick $w \in W, w \notin \text{span}(\{T_iv_1\})$. Then pick the linear map $T$
such that $Tv_1 = w$ and $Tv_i = 0$ for any $i > 1$. 3.5 assures us that this
map exists. It is not possible for any linear combination of the $T_i$ to map
$v_1$ to $w$ because we picked it outside the span of the $T_iv_1$. Hence, no
list of $T_i$ spans $\mathcal{L}(V,W)$, making it infinite-dimensional.

\problem{13}

By LD, there exist $a_i$ not all 0 such that $\sum a_iv_i = 0$. Then any linear
map $T$ must satisfy $\sum a_iTv_i = 0$.

For any $i$ where $a_i = 0$, pick $w_i = 0$. For any $i$ where $a_i \neq 0$,
pick $w_i = (1/a_i)w$ for some $w \neq 0$ (which is possible since $W \neq
\{0\}$). Then any linear map $T$ where $Tv_i = w_i$ would have to satisfy $\sum
a_iw_i = 0$, or $nw = 0$, where $n$ is the number of $a_i$ where $a_i \neq 0$.
$n > 0$ and $w \neq 0$, so this is impossible; hence no $T$ can satisfy $Tv_i =
w_i$ for this choice of $w_i$.

\problem{14}

Let $v_1, \ldots, v_n$ be a basis of $V$, with $n \geq 2$. Consider the two
maps:
\begin{align*}
  S(a_1v_1 + a_2v_2 + \ldots + a_nv_n) & = a_2v_1 + a_1v_2 \\
  T(a_1v_1 + a_2v_2 + \ldots + a_nv_n) & = a_1v_1
\end{align*}

In other words, both $S$ and $T$ zero out all but the terms for the first two
basis vectors, $S$ switches the coefficients of $v_1$ and $v_2$, and $T$
additionally zeroes out the coefficient of $v_2$.

These are both linear: clearly applying either to a sum or a scalar multiple
passes the coefficients and scaling factor onto their respective results. But:
\begin{align*}
  ST(a_1v_1 + a_2v_2) & = a_1v_2 \\
  TS(a_1v_1 + a_2v_2) & = a_2v_1
\end{align*}


\end{document}